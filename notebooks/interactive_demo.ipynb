{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cd .."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load packages\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import random\n",
    "import yaml\n",
    "from munch import Munch\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.ASR.models import ASRCNN\n",
    "from utils.JDC.model import JDCNet\n",
    "from models import Generator, MappingNetwork, StyleEncoder\n",
    "import soundfile as sf\n",
    "import IPython.display as ipd\n",
    "import pyworld\n",
    "from tqdm import tqdm\n",
    "% matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "speakers = ['F101', 'F102', 'F103', 'F104', 'F105', 'F106', 'F107', 'F108', 'F109', 'F110',\n",
    "            'M101', 'M102', 'M103', 'M104', 'M105', 'M106', 'M107', 'M108', 'M109', 'M110',\n",
    "            'FAF', 'FFS', 'FKM', 'FKN', 'FKS', 'FMS', 'FSU', 'FTK', 'FYM', 'FYN',\n",
    "            'MAU', 'MHT', 'MMS', 'MMY', 'MNM', 'MSH', 'MTK', 'MTM', 'MTT', 'MXM']\n",
    "print(len(speakers))\n",
    "to_mel = torchaudio.transforms.MelSpectrogram(\n",
    "    n_mels=80, n_fft=2048, win_length=1200, hop_length=300)\n",
    "mean, std = -4, 4\n",
    "\n",
    "\n",
    "def preprocess(wave):\n",
    "    wave_tensor = torch.from_numpy(wave).float()\n",
    "    mel_tensor = to_mel(wave_tensor)\n",
    "    mel_tensor = (torch.log(1e-5 + mel_tensor.unsqueeze(0)) - mean) / std\n",
    "    return mel_tensor\n",
    "\n",
    "\n",
    "def build_model(model_params={}):\n",
    "    args = Munch(model_params)\n",
    "    generator = Generator(args.dim_in, args.style_dim, args.max_conv_dim, w_hpf=args.w_hpf, F0_channel=args.F0_channel)\n",
    "    mapping_network = MappingNetwork(args.latent_dim, args.style_dim, args.num_domains, hidden_dim=args.max_conv_dim)\n",
    "    style_encoder = StyleEncoder(args.dim_in, args.style_dim, args.num_domains, args.max_conv_dim)\n",
    "\n",
    "    nets_ema = Munch(generator=generator,\n",
    "                     mapping_network=mapping_network,\n",
    "                     style_encoder=style_encoder)\n",
    "\n",
    "    return nets_ema\n",
    "\n",
    "\n",
    "def compute_style(model, speaker_dicts):\n",
    "    reference_embeddings = {}\n",
    "    for key, (path, speaker) in speaker_dicts.items():\n",
    "        if path == \"\":\n",
    "            label = torch.LongTensor([speaker]).to('cuda')\n",
    "            latent_dim = model.mapping_network.shared[0].in_features\n",
    "            ref = model.mapping_network(torch.randn(1, latent_dim).to('cuda'), label)\n",
    "        else:\n",
    "            print(path)\n",
    "            wave, sr = librosa.load(path, sr=24000)\n",
    "            audio, index = librosa.effects.trim(wave, top_db=30)\n",
    "            if sr != 24000:\n",
    "                wave = librosa.resample(wave, sr, 24000)\n",
    "            mel_tensor = preprocess(wave).to('cuda')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                label = torch.LongTensor([speaker])\n",
    "                ref = model.style_encoder(mel_tensor.unsqueeze(1), label)\n",
    "        reference_embeddings[key] = (ref, label)\n",
    "\n",
    "    return reference_embeddings\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load F0 model\n",
    "F0_model = JDCNet(num_class=1, seq_len=192)\n",
    "params = torch.load(\"Utils/JDC/bst.t7\")['net']\n",
    "F0_model.load_state_dict(params)\n",
    "_ = F0_model.eval()\n",
    "F0_model = F0_model.to('cuda')\n",
    "\n",
    "# load vocoder\n",
    "from parallel_wavegan.utils import load_model\n",
    "\n",
    "vocoder = load_model(\"Vocoder/checkpoint-400000steps.pkl\").to('cuda').eval()\n",
    "vocoder.remove_weight_norm()\n",
    "_ = vocoder.eval()\n",
    "\n",
    "# load starganv2\n",
    "model_path = 'Models/atr/epoch_00032.pth'\n",
    "\n",
    "with open('Configs/config.yml') as f:\n",
    "    starganv2_config = yaml.safe_load(f)\n",
    "starganv2 = build_model(model_params=starganv2_config[\"model_params\"])\n",
    "params = torch.load(model_path, map_location='cpu')\n",
    "print(\"Epochs:\", params[\"epochs\"])\n",
    "\n",
    "params = params['model_ema']\n",
    "_ = [starganv2[key].load_state_dict(params[key]) for key in starganv2]\n",
    "_ = [starganv2[key].eval() for key in starganv2]\n",
    "starganv2.style_encoder = starganv2.style_encoder.to('cuda')\n",
    "starganv2.mapping_network = starganv2.mapping_network.to('cuda')\n",
    "starganv2.generator = starganv2.generator.to('cuda')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "speakers_normal = ['F101', 'F102', 'F103', 'F104', 'F105', 'F106', 'F107', 'F108', 'F109', 'F110',\n",
    "                   'M101', 'M102', 'M103', 'M104', 'M105', 'M106', 'M107', 'M108', 'M109', 'M110']\n",
    "speakers_prof = ['FAF', 'FFS', 'FKM', 'FKN', 'FKS', 'FMS', 'FSU', 'FTK', 'FYM', 'FYN',\n",
    "                 'MAU', 'MHT', 'MMS', 'MMY', 'MNM', 'MSH', 'MTK', 'MTM', 'MTT', 'MXM']\n",
    "speakers_male_prof = ['MAU', 'MHT', 'MMS', 'MMY', 'MNM', 'MSH', 'MTK', 'MTM', 'MTT', 'MXM']\n",
    "speakers_female_prof = ['FAF', 'FFS', 'FKM', 'FKN', 'FKS', 'FMS', 'FSU', 'FTK', 'FYM', 'FYN']\n",
    "speakers_male_normal = ['M101', 'M102', 'M103', 'M104', 'M105', 'M106', 'M107', 'M108', 'M109', 'M110']\n",
    "speakers_female_normal = ['F101', 'F102', 'F103', 'F104', 'F105', 'F106', 'F107', 'F108', 'F109', 'F110']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate speaker embedding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# no reference, using mapping network\n",
    "speaker_dicts = {}\n",
    "for s in speakers:\n",
    "    speaker_dicts[s] = (\"data/ATR_processed/wav24/%s/1.wav\" % s,\n",
    "                        speakers.index(s))\n",
    "reference_embeddings = compute_style(starganv2, speaker_dicts)\n",
    "embedding = np.array([reference_embeddings[k][0].squeeze().cpu().numpy() for k in reference_embeddings])\n",
    "label = list(reference_embeddings.keys())\n",
    "print(embedding.shape)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=6, svd_solver='arpack')\n",
    "emb_pca = pca.fit_transform(embedding)\n",
    "print(emb_pca.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Interactive demo"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interact_manual\n",
    "ref, _ = reference_embeddings['M105']\n",
    "print(ref.shape)\n",
    "ref_pca = pca.transform(ref.cpu().numpy())\n",
    "ref_pca = ref_pca.squeeze()\n",
    "ref_pca_max = np.max(emb_pca, axis=0)\n",
    "ref_pca_min = np.min(emb_pca, axis=0)\n",
    "# ref_pca_max[2]\n",
    "print(ref_pca_min.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wav_path = \"data/samples/M105/M105SF_A01.AD.wav\"\n",
    "audio, source_sr = librosa.load(wav_path, sr=24000)\n",
    "source = preprocess(audio).to('cuda:0')\n",
    "with torch.no_grad():\n",
    "    f0_feat = F0_model.get_feature_GAN(source.unsqueeze(1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@interact(emb_dim_0=(ref_pca_min[0], ref_pca_max[0], 0.1),\n",
    "#           emb_dim_1=(ref_pca_min[1], ref_pca_max[1], 0.1),\n",
    "          emb_dim_1=(-10, 10, 0.1),\n",
    "          emb_dim_2=(ref_pca_min[2], ref_pca_max[2], 0.1),\n",
    "          emb_dim_3=(ref_pca_min[3], ref_pca_max[3], 0.1),\n",
    "          emb_dim_4=(ref_pca_min[4], ref_pca_max[4], 0.1))\n",
    "def voice_conversion(emb_dim_0=ref_pca[0],\n",
    "                     emb_dim_1=ref_pca[1],\n",
    "                     emb_dim_2=ref_pca[2],\n",
    "                     emb_dim_3=ref_pca[3],\n",
    "                     emb_dim_4=ref_pca[4]):\n",
    "    tar_emb_pca = ref_pca\n",
    "    tar_emb_pca[:5] = np.array([emb_dim_0, emb_dim_1, emb_dim_2, emb_dim_3, emb_dim_4])\n",
    "    tar_emb = torch.from_numpy(pca.inverse_transform(tar_emb_pca)).float().cuda()[None,]\n",
    "    with torch.no_grad():\n",
    "        global mel_hat\n",
    "        mel_hat = starganv2.generator(source.unsqueeze(1), tar_emb, F0=f0_feat).squeeze().cpu().numpy()\n",
    "    plt.figure(dpi=100, figsize=(5, 3))\n",
    "    plt.imshow(mel_hat, aspect='auto', origin='lower', cmap='inferno')\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Converted\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Synthesize waveform"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = torch.from_numpy(mel_hat).float()[None,].transpose(-1, -2).squeeze().to('cuda')\n",
    "    y_out = vocoder.inference(out)\n",
    "    y_out = y_out.view(-1).cpu().numpy()\n",
    "print(\"Original\")\n",
    "ipd.display(ipd.Audio(audio, rate=24000))\n",
    "print(\"Converted\")\n",
    "ipd.display(ipd.Audio(y_out, rate=24000))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}