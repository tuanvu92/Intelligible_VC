{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cd.."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load packages\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import random\n",
    "import yaml\n",
    "from munch import Munch\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.ASR.models import ASRCNN\n",
    "from utils.JDC.model import JDCNet\n",
    "from models import Generator, MappingNetwork, StyleEncoder\n",
    "import soundfile as sf\n",
    "import IPython.display as ipd\n",
    "import pyworld\n",
    "from tqdm import tqdm\n",
    "% matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "speakers = ['F101', 'F102', 'F103', 'F104', 'F105', 'F106', 'F107', 'F108', 'F109', 'F110',\n",
    "            'M101', 'M102', 'M103', 'M104', 'M105', 'M106', 'M107', 'M108', 'M109', 'M110',\n",
    "            'FAF', 'FFS', 'FKM', 'FKN', 'FKS', 'FMS', 'FSU', 'FTK', 'FYM', 'FYN',\n",
    "            'MAU', 'MHT', 'MMS', 'MMY', 'MNM', 'MSH', 'MTK', 'MTM', 'MTT', 'MXM']\n",
    "print(len(speakers))\n",
    "to_mel = torchaudio.transforms.MelSpectrogram(\n",
    "    n_mels=80, n_fft=2048, win_length=1200, hop_length=300)\n",
    "mean, std = -4, 4\n",
    "\n",
    "\n",
    "def preprocess(wave):\n",
    "    wave_tensor = torch.from_numpy(wave).float()\n",
    "    mel_tensor = to_mel(wave_tensor)\n",
    "    mel_tensor = (torch.log(1e-5 + mel_tensor.unsqueeze(0)) - mean) / std\n",
    "    return mel_tensor\n",
    "\n",
    "\n",
    "def build_model(model_params={}):\n",
    "    args = Munch(model_params)\n",
    "    generator = Generator(args.dim_in, args.style_dim, args.max_conv_dim, w_hpf=args.w_hpf, F0_channel=args.F0_channel)\n",
    "    mapping_network = MappingNetwork(args.latent_dim, args.style_dim, args.num_domains, hidden_dim=args.max_conv_dim)\n",
    "    style_encoder = StyleEncoder(args.dim_in, args.style_dim, args.num_domains, args.max_conv_dim)\n",
    "\n",
    "    nets_ema = Munch(generator=generator,\n",
    "                     mapping_network=mapping_network,\n",
    "                     style_encoder=style_encoder)\n",
    "\n",
    "    return nets_ema\n",
    "\n",
    "\n",
    "def compute_style(model, speaker_dicts):\n",
    "    reference_embeddings = {}\n",
    "    for key, (path, speaker) in speaker_dicts.items():\n",
    "        if path == \"\":\n",
    "            label = torch.LongTensor([speaker]).to('cuda')\n",
    "            latent_dim = model.mapping_network.shared[0].in_features\n",
    "            ref = model.mapping_network(torch.randn(1, latent_dim).to('cuda'), label)\n",
    "        else:\n",
    "            print(path)\n",
    "            wave, sr = librosa.load(path, sr=24000)\n",
    "            audio, index = librosa.effects.trim(wave, top_db=30)\n",
    "            if sr != 24000:\n",
    "                wave = librosa.resample(wave, sr, 24000)\n",
    "            mel_tensor = preprocess(wave).to('cuda')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                label = torch.LongTensor([speaker])\n",
    "                ref = model.style_encoder(mel_tensor.unsqueeze(1), label)\n",
    "        reference_embeddings[key] = (ref, label)\n",
    "\n",
    "    return reference_embeddings\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# load F0 model\n",
    "F0_model = JDCNet(num_class=1, seq_len=192)\n",
    "params = torch.load(\"utils/JDC/bst.t7\")['net']\n",
    "F0_model.load_state_dict(params)\n",
    "_ = F0_model.eval()\n",
    "F0_model = F0_model.to('cuda')\n",
    "\n",
    "# load vocoder\n",
    "from parallel_wavegan.utils import load_model\n",
    "\n",
    "vocoder = load_model(\"Vocoder/checkpoint-400000steps.pkl\").to('cuda').eval()\n",
    "vocoder.remove_weight_norm()\n",
    "_ = vocoder.eval()\n",
    "\n",
    "# load starganv2\n",
    "model_path = 'Models/atr/epoch_00032.pth'\n",
    "\n",
    "with open('Configs/config.yml') as f:\n",
    "    starganv2_config = yaml.safe_load(f)\n",
    "starganv2 = build_model(model_params=starganv2_config[\"model_params\"])\n",
    "params = torch.load(model_path, map_location='cpu')\n",
    "print(\"Epochs:\", params[\"epochs\"])\n",
    "\n",
    "params = params['model_ema']\n",
    "_ = [starganv2[key].load_state_dict(params[key]) for key in starganv2]\n",
    "_ = [starganv2[key].eval() for key in starganv2]\n",
    "starganv2.style_encoder = starganv2.style_encoder.to('cuda')\n",
    "starganv2.mapping_network = starganv2.mapping_network.to('cuda')\n",
    "starganv2.generator = starganv2.generator.to('cuda')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "speakers_normal = ['F101', 'F102', 'F103', 'F104', 'F105', 'F106', 'F107', 'F108', 'F109', 'F110',\n",
    "            'M101', 'M102', 'M103', 'M104', 'M105', 'M106', 'M107', 'M108', 'M109', 'M110']\n",
    "speakers_prof = ['FAF', 'FFS', 'FKM', 'FKN', 'FKS', 'FMS', 'FSU', 'FTK', 'FYM', 'FYN',\n",
    "            'MAU', 'MHT', 'MMS', 'MMY', 'MNM', 'MSH', 'MTK', 'MTM', 'MTT', 'MXM']\n",
    "speakers_male_prof = ['MAU', 'MHT', 'MMS', 'MMY', 'MNM', 'MSH', 'MTK', 'MTM', 'MTT', 'MXM']\n",
    "speakers_female_prof = ['FAF', 'FFS', 'FKM', 'FKN', 'FKS', 'FMS', 'FSU', 'FTK', 'FYM', 'FYN']\n",
    "speakers_male_normal = ['M101', 'M102', 'M103', 'M104', 'M105', 'M106', 'M107', 'M108', 'M109', 'M110']\n",
    "speakers_female_normal = ['F101', 'F102', 'F103', 'F104', 'F105', 'F106', 'F107', 'F108', 'F109', 'F110']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calculate speaker embedding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def scale_db(y, target_dB_FS=-25, eps=1e-6):\n",
    "    rms = np.sqrt(np.mean(y ** 2))\n",
    "    scalar = 10 ** (target_dB_FS / 20) / (rms + eps)\n",
    "    y *= scalar\n",
    "    return y\n",
    "\n",
    "# no reference, using mapping network\n",
    "speaker_dicts = {}\n",
    "for s in speakers:\n",
    "    speaker_dicts[s] = (\"data/ATR_processed/wav24/%s/1.wav\" % s,\n",
    "                        speakers.index(s))\n",
    "reference_embeddings = compute_style(starganv2, speaker_dicts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embedding = np.array([reference_embeddings[k][0].squeeze().cpu().numpy() for k in reference_embeddings])\n",
    "label = list(reference_embeddings.keys())\n",
    "print(embedding.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=6, svd_solver='arpack')\n",
    "emb_pca = pca.fit_transform(embedding)\n",
    "print(emb_pca.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib.transforms as transforms\n",
    "\n",
    "def confidence_ellipse(x, y, ax, n_std=3.0, facecolor='none', **kwargs):\n",
    "    \"\"\"\n",
    "    Create a plot of the covariance confidence ellipse of *x* and *y*.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y : array-like, shape (n, )\n",
    "        Input data.\n",
    "\n",
    "    ax : matplotlib.axes.Axes\n",
    "        The axes object to draw the ellipse into.\n",
    "\n",
    "    n_std : float\n",
    "        The number of standard deviations to determine the ellipse's radiuses.\n",
    "\n",
    "    **kwargs\n",
    "        Forwarded to `~matplotlib.patches.Ellipse`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.patches.Ellipse\n",
    "    \"\"\"\n",
    "    if x.size != y.size:\n",
    "        raise ValueError(\"x and y must be the same size\")\n",
    "\n",
    "    cov = np.cov(x, y)\n",
    "    pearson = cov[0, 1]/np.sqrt(cov[0, 0] * cov[1, 1])\n",
    "    # Using a special case to obtain the eigenvalues of this\n",
    "    # two-dimensionl dataset.\n",
    "    ell_radius_x = np.sqrt(1 + pearson)\n",
    "    ell_radius_y = np.sqrt(1 - pearson)\n",
    "    ellipse = Ellipse((0, 0), width=ell_radius_x * 1.7, height=ell_radius_y * 1.7,\n",
    "                      facecolor=facecolor, **kwargs)\n",
    "\n",
    "    # Calculating the stdandard deviation of x from\n",
    "    # the squareroot of the variance and multiplying\n",
    "    # with the given number of standard deviations.\n",
    "    scale_x = np.sqrt(cov[0, 0]) * n_std\n",
    "    mean_x = np.mean(x)\n",
    "\n",
    "    # calculating the stdandard deviation of y ...\n",
    "    scale_y = np.sqrt(cov[1, 1]) * n_std\n",
    "    mean_y = np.mean(y)\n",
    "    ax.scatter([mean_x], [mean_y], marker='X', c='C3', edgecolor=\"None\", alpha=0.8, zorder=3, s=8)\n",
    "    transf = transforms.Affine2D() \\\n",
    "        .rotate_deg(45) \\\n",
    "        .scale(scale_x, scale_y) \\\n",
    "        .translate(mean_x, mean_y)\n",
    "\n",
    "    ellipse.set_transform(transf + ax.transData)\n",
    "    return ax.add_patch(ellipse)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,4), dpi=250)\n",
    "idx = [i for i, l in enumerate(label) if l in speakers_male_prof]\n",
    "confidence_ellipse(emb_pca[idx, 0], emb_pca[idx, 1], ax, facecolor='C0', alpha=0.2)\n",
    "idx = [i for i, l in enumerate(label) if l in speakers_female_prof]\n",
    "confidence_ellipse(emb_pca[idx, 0], emb_pca[idx, 1], ax, facecolor='C1',  alpha=0.2)\n",
    "\n",
    "\n",
    "idx = [i for i, l in enumerate(label) if l in speakers_female_prof]\n",
    "ax.scatter(emb_pca[idx, 0], emb_pca[idx, 1], zorder=3,  marker='^', edgecolor='none', c='C0', alpha=0.7, label='Female announcer')\n",
    "print(np.mean(emb_pca[idx], axis=0))\n",
    "\n",
    "idx = [i for i, l in enumerate(label) if l in speakers_male_prof]\n",
    "ax.scatter(emb_pca[idx, 0], emb_pca[idx, 1], zorder=3,  marker='v', c='C0', edgecolor='none', alpha=0.7, label='Male announcer')\n",
    "\n",
    "\n",
    "idx = [i for i, l in enumerate(label) if l in speakers_female_normal]\n",
    "ax.scatter(emb_pca[idx, 0], emb_pca[idx, 1], zorder=3, marker='^', c='C1', edgecolor='none', alpha=0.7, label='Female non-expert')\n",
    "\n",
    "idx = [i for i, l in enumerate(label) if l in speakers_male_normal]\n",
    "ax.scatter(emb_pca[idx, 0], emb_pca[idx, 1], zorder=3, marker='v', c='C1', edgecolor='none', alpha=0.7, label='Male non-expert')\n",
    "\n",
    "\n",
    "idx = [i for i, l in enumerate(label) if l in speakers_male_normal]\n",
    "confidence_ellipse(emb_pca[idx, 0], emb_pca[idx, 1], ax, facecolor='C2', alpha=0.2)\n",
    "idx = [i for i, l in enumerate(label) if l in speakers_female_normal]\n",
    "confidence_ellipse(emb_pca[idx, 0], emb_pca[idx, 1], ax, facecolor='C3',  alpha=0.2)\n",
    "\n",
    "# ax.scatter(emb_pca[[14,30], 0], emb_pca[[14,30], 1], zorder=3, marker='v', c=\"None\", edgecolor='r', alpha=0.7)\n",
    "plt.xlabel(\"Dimension 0\")\n",
    "plt.ylabel(\"Dimension 1\")\n",
    "for l, e in zip(label, emb_pca):\n",
    "    if l == \"MAU\" or l == \"M105\" :\n",
    "        plt.annotate(l, xy=[e[0], e[1]], xytext=[e[0]-3.5, e[1]], ha='center', va='center', arrowprops=dict(arrowstyle=\"->\"))\n",
    "ax.grid(zorder=5, linestyle='--')\n",
    "ax.legend(ncol=2, loc='lower center', bbox_to_anchor=(0.5, 1), fontsize=8)\n",
    "plt.savefig(\"emb_pca.svg\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}