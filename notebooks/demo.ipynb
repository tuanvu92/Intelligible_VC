{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load packages\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import random\n",
    "import yaml\n",
    "from munch import Munch\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.ASR.models import ASRCNN\n",
    "from utils.JDC.model import JDCNet\n",
    "from models import Generator, MappingNetwork, StyleEncoder\n",
    "import soundfile as sf\n",
    "import IPython.display as ipd\n",
    "import pyworld\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "speakers = ['F101', 'F102', 'F103', 'F104', 'F105', 'F106', 'F107', 'F108', 'F109', 'F110',\n",
    "            'M101', 'M102', 'M103', 'M104', 'M105', 'M106', 'M107', 'M108', 'M109', 'M110',\n",
    "            'FAF', 'FFS', 'FKM', 'FKN', 'FKS', 'FMS', 'FSU', 'FTK', 'FYM', 'FYN',\n",
    "            'MAU', 'MHT', 'MMS', 'MMY', 'MNM', 'MSH', 'MTK', 'MTM', 'MTT', 'MXM']\n",
    "print(len(speakers))\n",
    "to_mel = torchaudio.transforms.MelSpectrogram(\n",
    "    n_mels=80, n_fft=2048, win_length=1200, hop_length=300)\n",
    "mean, std = -4, 4\n",
    "\n",
    "def preprocess(wave):\n",
    "    wave_tensor = torch.from_numpy(wave).float()\n",
    "    mel_tensor = to_mel(wave_tensor)\n",
    "    mel_tensor = (torch.log(1e-5 + mel_tensor.unsqueeze(0)) - mean) / std\n",
    "    return mel_tensor\n",
    "\n",
    "def build_model(model_params={}):\n",
    "    args = Munch(model_params)\n",
    "    generator = Generator(args.dim_in, args.style_dim, args.max_conv_dim, w_hpf=args.w_hpf, F0_channel=args.F0_channel)\n",
    "    mapping_network = MappingNetwork(args.latent_dim, args.style_dim, args.num_domains, hidden_dim=args.max_conv_dim)\n",
    "    style_encoder = StyleEncoder(args.dim_in, args.style_dim, args.num_domains, args.max_conv_dim)\n",
    "\n",
    "    nets_ema = Munch(generator=generator,\n",
    "                     mapping_network=mapping_network,\n",
    "                     style_encoder=style_encoder)\n",
    "\n",
    "    return nets_ema\n",
    "\n",
    "def compute_style(model, speaker_dicts):\n",
    "    reference_embeddings = {}\n",
    "    for key, (path, speaker) in speaker_dicts.items():\n",
    "        if path == \"\":\n",
    "            label = torch.LongTensor([speaker]).to('cuda')\n",
    "            latent_dim = model.mapping_network.shared[0].in_features\n",
    "            ref = model.mapping_network(torch.randn(1, latent_dim).to('cuda'), label)\n",
    "        else:\n",
    "            print(path)\n",
    "            wave, sr = librosa.load(path, sr=24000)\n",
    "            audio, index = librosa.effects.trim(wave, top_db=30)\n",
    "            if sr != 24000:\n",
    "                wave = librosa.resample(wave, sr, 24000)\n",
    "            mel_tensor = preprocess(wave).to('cuda')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                label = torch.LongTensor([speaker])\n",
    "                ref = model.style_encoder(mel_tensor.unsqueeze(1), label)\n",
    "        reference_embeddings[key] = (ref, label)\n",
    "\n",
    "    return reference_embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load F0 model\n",
    "\n",
    "F0_model = JDCNet(num_class=1, seq_len=192)\n",
    "params = torch.load(\"utils/JDC/bst.t7\")['net']\n",
    "F0_model.load_state_dict(params)\n",
    "_ = F0_model.eval()\n",
    "F0_model = F0_model.to('cuda')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load vocoder\n",
    "from parallel_wavegan.utils import load_model\n",
    "vocoder = load_model(\"Vocoder/checkpoint-400000steps.pkl\").to('cuda').eval()\n",
    "vocoder.remove_weight_norm()\n",
    "_ = vocoder.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load starganv2\n",
    "\n",
    "model_path = 'Models/atr/epoch_00032.pth'\n",
    "\n",
    "with open('Configs/config.yml') as f:\n",
    "    starganv2_config = yaml.safe_load(f)\n",
    "starganv2 = build_model(model_params=starganv2_config[\"model_params\"])\n",
    "params = torch.load(model_path, map_location='cpu')\n",
    "print(\"Epochs:\", params[\"epochs\"])\n",
    "\n",
    "params = params['model_ema']\n",
    "_ = [starganv2[key].load_state_dict(params[key]) for key in starganv2]\n",
    "_ = [starganv2[key].eval() for key in starganv2]\n",
    "starganv2.style_encoder = starganv2.style_encoder.to('cuda')\n",
    "starganv2.mapping_network = starganv2.mapping_network.to('cuda')\n",
    "starganv2.generator = starganv2.generator.to('cuda')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conversion"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def scale_db(y, target_dB_FS=-25, eps=1e-6):\n",
    "    rms = np.sqrt(np.mean(y ** 2))\n",
    "    scalar = 10 ** (target_dB_FS / 20) / (rms + eps)\n",
    "    y *= scalar\n",
    "    return y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# no reference, using mapping network\n",
    "speaker_dicts = {}\n",
    "for s in speakers:\n",
    "    speaker_dicts[s] = (\"data/ATR_processed/wav24/%s/1.wav\" % s,\n",
    "                        speakers.index(s))\n",
    "reference_embeddings = compute_style(starganv2, speaker_dicts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# conversion\n",
    "import time\n",
    "start = time.time()\n",
    "wav_path = \"data/samples/M105/M105SF_A01.AD.wav\"\n",
    "audio, source_sr = librosa.load(wav_path, sr=24000)\n",
    "print(audio.shape)\n",
    "audio = audio / np.max(np.abs(audio))\n",
    "ipd.display(ipd.Audio(audio, rate=24000))\n",
    "source = preprocess(audio).to('cuda:0')\n",
    "keys = []\n",
    "converted_samples = {}\n",
    "reconstructed_samples = {}\n",
    "converted_mels = {}\n",
    "style_pred = {}\n",
    "for key, (ref, _) in reference_embeddings.items():\n",
    "    with torch.no_grad():\n",
    "        f0_feat = F0_model.get_feature_GAN(source.unsqueeze(1))\n",
    "        out = starganv2.generator(source.unsqueeze(1), ref, F0=f0_feat)\n",
    "        c = out.transpose(-1, -2).squeeze().to('cuda')\n",
    "        y_out = vocoder.inference(c)\n",
    "        y_out = y_out.view(-1).cpu()\n",
    "        recon = None\n",
    "        if key not in speaker_dicts or speaker_dicts[key][0] == \"\":\n",
    "            recon = None\n",
    "        else:\n",
    "            wave, sr = librosa.load(speaker_dicts[key][0], sr=24000)\n",
    "            mel = preprocess(wave)\n",
    "            c = mel.transpose(-1, -2).squeeze().to('cuda')\n",
    "            recon = vocoder.inference(c)\n",
    "            recon = recon.view(-1).cpu().numpy()\n",
    "\n",
    "    converted_samples[key] = y_out.numpy()\n",
    "    reconstructed_samples[key] = recon\n",
    "\n",
    "    converted_mels[key] = out\n",
    "\n",
    "    keys.append(key)\n",
    "end = time.time()\n",
    "print('total processing time: %.3f sec' % (end - start) )\n",
    "\n",
    "\n",
    "for key, wave in converted_samples.items():\n",
    "    print('Converted: %s' % key)\n",
    "    ipd.display(ipd.Audio(wave, rate=24000))\n",
    "    print('Reference (vocoder): %s' % key)\n",
    "    if reconstructed_samples[key] is not None:\n",
    "        ipd.display(ipd.Audio(reconstructed_samples[key], rate=24000))\n",
    "\n",
    "print('Original (vocoder):')\n",
    "wave, sr = librosa.load(wav_path, sr=24000)\n",
    "mel = preprocess(wave)\n",
    "c = mel.transpose(-1, -2).squeeze().to('cuda')\n",
    "with torch.no_grad():\n",
    "    recon = vocoder.inference(c)\n",
    "    recon = recon.view(-1).cpu().numpy()\n",
    "ipd.display(ipd.Audio(recon, rate=24000))\n",
    "print('Original:')\n",
    "ipd.display(ipd.Audio(wav_path, rate=24000))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}